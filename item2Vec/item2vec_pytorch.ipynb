{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data as Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import collections\n",
    "import math\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class item2VecDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, centers, contexts, negatives):\n",
    "        assert len(centers) == len(contexts) == len(negatives)\n",
    "        super(item2VecDataset, self).__init__()\n",
    "        self.centers = centers\n",
    "        self.contexts = contexts\n",
    "        self.negatives = negatives\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.centers[index], self.contexts[index], self.negatives[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryCrossEntropyLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, mask = None):\n",
    "        inputs, targets, mask = inputs.float(), targets.float(), mask.float()\n",
    "        res = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction = 'none', weight = mask)\n",
    "        return res.mean(dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embedding1 = nn.Embedding(num_embeddings = size, embedding_dim = embed_size)\n",
    "        self.embedding2 = nn.Embedding(num_embeddings = size, embedding_dim = embed_size)\n",
    "\n",
    "    def forward(self, context, center_negative):\n",
    "        v = self.embedding1(context)\n",
    "        v = v.mean(dim = 0)\n",
    "        u = self.embedding2(center_negative)\n",
    "        pred = torch.bmm(v, u.permute(0, 2, 1))\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_gram(center, contexts_negatives, embed_v, embed_u):\n",
    "    v = embed_v(center)\n",
    "    u = embed_u(contexts_negatives)\n",
    "    pred = torch.bmm(v, u.permute(0, 2, 1))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_item2Vec(net, lr, num_epochs, loss, data_iter):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print('train on', device)\n",
    "    net = net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr = lr)\n",
    "    for epoch in range(num_epochs):\n",
    "        start, l_sum, n = time.time(), 0.0, 0\n",
    "        for batch in data_iter:\n",
    "            center, context_negative, mask, label = [d.to(device) for d in batch]\n",
    "#            print(center.shape, context_negative.shape)\n",
    "            pred = skip_gram(center, context_negative, net[0], net[1])\n",
    "#            print(pred.shape, mask.shape, label.shape)\n",
    "            l = (loss(pred.view(label.shape), label, mask.view(label.shape)) * mask.shape[1] / mask.float().sum(dim = 1)).mean()\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            l_sum += l.cpu().item()\n",
    "            n += 1\n",
    "\n",
    "        print('epoch %d, loss %f, time %.2fs' % (epoch + 1, l_sum / n, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus(path):\n",
    "    ratings = pd.read_csv(path)\n",
    "    all_rating = pd.read_csv(\"~/Data/clean_rating4.csv\").drop([\"id\"], axis = 1)\n",
    "#     print(ratings[ratings.anime_id == 31687])\n",
    "    positive_rating = ratings\n",
    "    all_anime_list = all_rating['anime_id'].tolist()\n",
    "    anime_list = ratings[\"anime\"].tolist()\n",
    "    counter = collections.Counter(anime_list)\n",
    "\n",
    "    idx_to_anime = list(set(all_anime_list))\n",
    "    anime_to_idx = {anime_id: idx for idx, anime_id in enumerate(idx_to_anime)}\n",
    "    \n",
    "    gp = positive_rating.groupby(\"user\")\n",
    "    corpus = [list(map(lambda x: anime_to_idx[x], gp.get_group(user_id)['anime'].tolist())) for user_id, _ in gp]\n",
    "#     print(len(sum(corpus, [])))\n",
    "    corpus = [[aid for aid in st if subsampling(aid, counter, len(anime_list), idx_to_anime)] for st in corpus]\n",
    "#     print(len(sum(corpus, [])))\n",
    "    return corpus, anime_to_idx, idx_to_anime, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsampling(aid, counter, size, idx_to_anime):\n",
    "    return random.uniform(0, 1) < 1 - math.sqrt(1e-4 * size / counter[idx_to_anime[aid]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centers_and_contexts(corpus):\n",
    "    centers, contexts = [], []\n",
    "    for animes in corpus:\n",
    "        if len(animes) < 2:\n",
    "            continue\n",
    "        centers += animes\n",
    "        for idx, _ in enumerate(animes):\n",
    "            contexts.append(animes[:idx] + animes[idx + 1:])\n",
    "\n",
    "    return centers, contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_sampling(contexts, weights, K):\n",
    "    negatives, neg_candidates = [], []\n",
    "    all_animes = list(range(len(weights)))\n",
    "    for idx, context in enumerate(contexts):\n",
    "        if not idx % 10000: print(idx)\n",
    "        negs, i = [], 0\n",
    "        neg_candidates = random.choices(all_animes, weights, k = int(1e5))\n",
    "        while len(negs) < K:\n",
    "            if neg_candidates[i] in context:\n",
    "                i += 1\n",
    "                continue\n",
    "            else:\n",
    "                negs.append(neg_candidates[i])\n",
    "\n",
    "        negatives.append(negs)\n",
    "\n",
    "    return negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_batch(data):\n",
    "    max_len = max([len(context) + len(negative) for _, context, negative in data])\n",
    "    context_negatives, masks, labels, centers = [], [], [], []\n",
    "    for center, context, negative in data:\n",
    "        cur_len = len(context) + len(negative)\n",
    "        context_negatives.append(context + negative + [0] * (max_len - cur_len))\n",
    "        centers.append(center)\n",
    "        masks.append([1] * cur_len + [0] * (max_len - cur_len))\n",
    "        labels.append([1] * len(context) + [0] * (max_len - len(context)))\n",
    "\n",
    "    return (torch.tensor(centers).view(-1, 1), torch.tensor(context_negatives), torch.tensor(masks), torch.tensor(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "755217 4701 9775\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    anime = pd.read_csv('~/Data/anime.csv')\n",
    "    corpus, anime_to_idx, idx_to_anime, counter = get_corpus('~/Thesis/Data/train.csv')\n",
    "#     print()\n",
    "    print(sum([len(st) for st in corpus]), len(corpus), len(idx_to_anime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "755217 755217\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n"
     ]
    }
   ],
   "source": [
    "    centers, contexts = get_centers_and_contexts(corpus)\n",
    "    weights = [counter[aid] ** 0.75 for aid in idx_to_anime]\n",
    "    print(len(centers), len(contexts))\n",
    "    \n",
    "    negatives = negative_sampling(contexts, weights, 5)\n",
    "    print(len(centers), len(contexts), len(negatives))\n",
    "    \n",
    "    with open('negative.txt', 'w') as f:\n",
    "        f.write(str(negatives))\n",
    "        f.close()\n",
    "    \n",
    "    with open('centers.txt', 'w') as f:\n",
    "        f.write(str(centers))\n",
    "        f.close()\n",
    "    \n",
    "    with open('contexts.txt', 'w') as f:\n",
    "        f.write(str(contexts))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "755333 755333\n",
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-b37c3ab3bc42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#     print(len(negative))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mnegatives\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnegative_sampling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcenters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegatives\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-f8b8e19a8bff>\u001b[0m in \u001b[0;36mnegative_sampling\u001b[0;34m(contexts, weights, K)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mnegs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mneg_candidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_animes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mneg_candidates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/random.py\u001b[0m in \u001b[0;36mchoices\u001b[0;34m(self, population, weights, cum_weights, k)\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0mhi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcum_weights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         return [population[bisect(cum_weights, random() * total, 0, hi)]\n\u001b[0;32m--> 366\u001b[0;31m                 for i in range(k)]\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;31m## -------------------- real-valued distributions  -------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/random.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0mhi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcum_weights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         return [population[bisect(cum_weights, random() * total, 0, hi)]\n\u001b[0;32m--> 366\u001b[0;31m                 for i in range(k)]\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;31m## -------------------- real-valued distributions  -------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "        \n",
    "    batch_size = 512\n",
    "    num_workers = 0 if sys.platform.startswith('win32') else 4\n",
    "\n",
    "    dataset = item2VecDataset(centers, contexts, negatives)\n",
    "    data_iter = Data.DataLoader(dataset, batch_size, shuffle = True, collate_fn = select_batch, num_workers = num_workers)\n",
    "\n",
    "    embed_size = 100\n",
    "#    for batch in data_iter:\n",
    "#        for name, data in zip(['centers', 'context_negatives', 'masks', 'labels'], batch):\n",
    "#            print(name, data.shape)\n",
    "#        break\n",
    "    net = nn.Sequential(\n",
    "        nn.Embedding(num_embeddings = len(idx_to_anime), embedding_dim = embed_size),\n",
    "        nn.Embedding(num_embeddings = len(idx_to_anime), embedding_dim = embed_size) \n",
    "    )\n",
    "    train_item2Vec(net, 0.1, 30, BinaryCrossEntropyLoss(), data_iter)\n",
    "\n",
    "    torch.save(net.state_dict(), 'skip_gram_complete.pt')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_size = 100\n",
    "net = nn.Sequential(\n",
    "        nn.Embedding(num_embeddings = len(idx_to_anime), embedding_dim = embed_size),\n",
    "        nn.Embedding(num_embeddings = len(idx_to_anime), embedding_dim = embed_size) \n",
    "    )\n",
    "net.load_state_dict(torch.load(\"skip_gram_complete.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "31687",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-bb483a2ca58c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mta\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_anime\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0manime_to_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mta\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mst\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 31687"
     ]
    }
   ],
   "source": [
    "# W = net[0].weight.data\n",
    "# x = W[10]\n",
    "# print(anime[anime['anime_id'] == 5114])\n",
    "# cos = torch.matmul(W, x) / (torch.sum(W * W, dim = 1) * torch.sum(x * x) + 1e-9).sqrt()\n",
    "# _, topk = torch.topk(cos, k = 6)\n",
    "# topk = topk.cpu().numpy()\n",
    "# for i in topk[1:]:\n",
    "#     print('cosine sim = %.3f' % (cos[i]))\n",
    "#     print(anime[anime['anime_id'] == idx_to_anime[i]])\n",
    "    \n",
    "final = []\n",
    "test_df = pd.read_csv(\"~/Thesis/Data/test.csv\")\n",
    "users = list(set(test_df.user))\n",
    "for k in range(10):\n",
    "    precision = []\n",
    "    for user in users:\n",
    "        for i in range(10, -1, -1):\n",
    "            animes = test_df[(test_df.user == user) & (test_df.rating == i)].anime.tolist()\n",
    "            if animes:\n",
    "                anime = animes[0]\n",
    "                break\n",
    "                \n",
    "        \n",
    "        x = net[0].weight.data[anime_to_idx[anime]]\n",
    "        cos = torch.matmul(W, x) / (torch.sum(W * W, dim = 1) * torch.sum(x * x) + 1e-9).sqrt()\n",
    "        target_anime = test_df[(test_df.user == user) & (test_df.rating > k)].anime.tolist()\n",
    "        temp = []\n",
    "        \n",
    "        for ta in target_anime:\n",
    "            temp.append((ta, cos[anime_to_idx[ta]]))\n",
    "            \n",
    "        idx = [st[0] for st in sorted(temp, key = lambda x: x[1], reverse = True)[1:6]]    \n",
    "        both = set(idx) & set(target_anime)\n",
    "#         print(len(idx), len(both))\n",
    "        precision.append(len(both) / len(idx))\n",
    "        \n",
    "    final.append(np.mean(precision))\n",
    "    print(final[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
